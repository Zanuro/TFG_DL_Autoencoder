{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python38564bit129eeb57289c44939b4863da13de07dc",
   "display_name": "Python 3.8.5 64-bit"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "samples = ['The cat sat on the mat.', 'The dog ate my homework.']\n",
    "\n",
    "## Assigning unique index to word\n",
    "token_idx = {}\n",
    "for sample in samples:\n",
    "    for word in sample.split():\n",
    "        if word not in token_idx:\n",
    "            token_idx[word] = len(token_idx) + 1\n",
    "\n",
    "length = 10\n",
    "\n",
    "results = np.zeros(shape=(len(samples),length, max(token_idx.values()) +1))\n",
    "\n",
    "for i, sample in enumerate(samples):\n",
    "    for j, word in list(enumerate(sample.split()))[:length]:\n",
    "        idx = token_idx.get(word)\n",
    "        results[i,j,idx] = 1.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[[[1. 1. 1. ... 1. 1. 1.]\n  [1. 1. 1. ... 1. 1. 1.]\n  [1. 1. 1. ... 1. 1. 1.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[1. 1. 1. ... 1. 1. 1.]\n  [1. 1. 1. ... 1. 1. 1.]\n  [1. 1. 1. ... 1. 1. 1.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]]\n"
     ]
    }
   ],
   "source": [
    "## Character-level one-hot encoding\n",
    "\n",
    "import string\n",
    "\n",
    "samples = ['The cat sat on the mat.', 'The dog ate my homework.']\n",
    "chars = string.printable ## all chars\n",
    "token_idx = dict(zip(range(1, len(chars) +1), chars))\n",
    "\n",
    "length = 50\n",
    "\n",
    "results = np.zeros((len(samples), length, max(token_idx.keys()) + 1 ))\n",
    "\n",
    "for i, sample in enumerate(samples):\n",
    "    for j,character in enumerate(sample):\n",
    "        idx = token_idx.get(character)\n",
    "        results[i,j,idx] = 1.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[[0. 1. 1. ... 0. 0. 0.]\n [0. 1. 0. ... 0. 0. 0.]]\nFound 9 unique tokens\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "samples = ['The cat sat on the mat.', 'The dog ate my homework.']\n",
    "\n",
    "tokenizer = Tokenizer(num_words = 10000)\n",
    "tokenizer.fit_on_texts(samples) ## word index\n",
    "\n",
    "seq = tokenizer.texts_to_sequences(samples) ## palabras en vectores de indices\n",
    "\n",
    "one_hot_results = tokenizer.texts_to_matrix(samples, mode='binary') ## directamente a matriz\n",
    "\n",
    "word_idx = tokenizer.word_index\n",
    "print('Found %s unique tokens' % len(word_idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "##One hot hashing trick\n",
    "##Se realiza mediante una funcion de hashing. La principal ventaja es ahorrar espacio y poder hacer el encoding sin que se tenga todos los datos disponibles\n",
    "\n",
    "samples = ['The cat sat on the mat.', 'The dog ate my homework.']\n",
    "\n",
    "dimensions = 1000 ## vectores de tamano 1000\n",
    "length = 10\n",
    "\n",
    "results = np.zeros((len(samples), length, dimensions))\n",
    "for i,sample in enumerate(samples):\n",
    "    for j,word in list(enumerate(sample.split()))[:length]:\n",
    "        idx = abs(hash(word)) % dimensions ## index random entre 0 y 1000\n",
    "        results[i,j,idx] = 1.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Capa Embedding para dataset IMDB\n",
    "\n",
    "from keras.datasets import imdb\n",
    "from keras import preprocessing\n",
    "\n",
    "max_features = 10000\n",
    "length = 20\n",
    "\n",
    "(x_train, y_train) , (x_test, y_test) = imdb.load_data(num_words= max_features)\n",
    "\n",
    "x_train = preprocessing.sequence.pad_sequences(x_train,maxlen=length)\n",
    "x_test = preprocessing.sequence.pad_sequences(x_test,maxlen=length)\n",
    "##Train y test sera un tensor tipo (numero_seq, 20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model: \"sequential_1\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nembedding (Embedding)        (None, 20, 8)             80000     \n_________________________________________________________________\nflatten (Flatten)            (None, 160)               0         \n_________________________________________________________________\ndense (Dense)                (None, 1)                 161       \n=================================================================\nTotal params: 80,161\nTrainable params: 80,161\nNon-trainable params: 0\n_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Flatten, Dense, Embedding\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(10000,8,input_length = length))\n",
    "\n",
    "model.add(Flatten()) ## lo transforma en (10000,8*length)\n",
    "\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['acc'])\n",
    "\n",
    "model.summary()\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/10\n",
      "625/625 [==============================] - 2s 3ms/step - loss: 0.6783 - acc: 0.5968 - val_loss: 0.6409 - val_acc: 0.6844\n",
      "Epoch 2/10\n",
      "625/625 [==============================] - 2s 3ms/step - loss: 0.5592 - acc: 0.7473 - val_loss: 0.5339 - val_acc: 0.7280\n",
      "Epoch 3/10\n",
      "625/625 [==============================] - 2s 2ms/step - loss: 0.4670 - acc: 0.7867 - val_loss: 0.5042 - val_acc: 0.7416\n",
      "Epoch 4/10\n",
      "625/625 [==============================] - 2s 3ms/step - loss: 0.4242 - acc: 0.8080 - val_loss: 0.4955 - val_acc: 0.7550\n",
      "Epoch 5/10\n",
      "625/625 [==============================] - 1s 2ms/step - loss: 0.3960 - acc: 0.8229 - val_loss: 0.4943 - val_acc: 0.7554\n",
      "Epoch 6/10\n",
      "625/625 [==============================] - 2s 2ms/step - loss: 0.3739 - acc: 0.8357 - val_loss: 0.4991 - val_acc: 0.7580\n",
      "Epoch 7/10\n",
      "625/625 [==============================] - 1s 2ms/step - loss: 0.3550 - acc: 0.8456 - val_loss: 0.5035 - val_acc: 0.7544\n",
      "Epoch 8/10\n",
      "625/625 [==============================] - 1s 2ms/step - loss: 0.3377 - acc: 0.8553 - val_loss: 0.5110 - val_acc: 0.7552\n",
      "Epoch 9/10\n",
      "625/625 [==============================] - 1s 2ms/step - loss: 0.3209 - acc: 0.8652 - val_loss: 0.5181 - val_acc: 0.7538\n",
      "Epoch 10/10\n",
      "625/625 [==============================] - 1s 2ms/step - loss: 0.3054 - acc: 0.8729 - val_loss: 0.5243 - val_acc: 0.7454\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(x_train, \n",
    "                   y_train, \n",
    "                   epochs = 10,\n",
    "                   validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Solo empleando las 20 primeras palabras de cada review nos ha salido un accuracy de 87%.Seria mejor emplear redes conv 1d o capas recurrentes."
   ]
  }
 ]
}