{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import os\n",
    "import pandas as pd\n",
    "from pandas import DataFrame\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "#from sklearn.externals import joblib\n",
    "import seaborn as sns\n",
    "sns.set(color_codes=True)\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from numpy.random import seed\n",
    "\n",
    "#tf.logging.set_verbosity(tf.logging.ERROR)\n",
    "from keras.layers import Input, Dropout, Dense, LSTM, TimeDistributed, RepeatVector\n",
    "from keras.models import Model\n",
    "from keras import regularizers\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
    "from sklearn.metrics import confusion_matrix, precision_recall_curve, cohen_kappa_score, fbeta_score\n",
    "from sklearn.metrics import recall_score, classification_report, auc, roc_curve, log_loss\n",
    "LABELS = [\"Normal\",\"FMA\"]\n",
    "\n",
    "#set up graphic style in this case I am using the color scheme from xkcd.com\n",
    "from pylab import rcParams\n",
    "rcParams['figure.figsize'] = 14, 8.7 # Golden Mean\n",
    "LABELS = [\"Normal\",\"Fraud\"]\n",
    "col_list = [\"cerulean\",\"scarlet\"]# https://xkcd.com/color/rgb/\n",
    "sns.set(style='white', font_scale=1.75, palette=sns.xkcd_palette(col_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set random seed\n",
    "import tensorflow as tf\n",
    "tf.random.set_seed(123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logger = tf.get_logger()\n",
    "logger.setLevel(logging.ERROR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Cargar training y testing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar training y testing\n",
    "# Read in data and display first 5 rows\n",
    "test = pd.read_csv(\"Dataset_Test.csv\")\n",
    "# validation\n",
    "train = pd.read_csv(\"Dataset_Training.csv\")\n",
    "print('The shape of our train is:', train.shape)\n",
    "#test.head(5)\n",
    "train.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Seleccionar FMA en el test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train[train['FMA'] == 0] # Seleccion de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split = 1460 # Validation\n",
    "validation = train[-split:]\n",
    "train = train[:-split]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The last element contains the labels\n",
    "# Convertir Series to DataFrame (.to_frame())\n",
    "train_fma = train.iloc[:, -1]\n",
    "validation_fma = validation.iloc[:, -1]\n",
    "test_fma = test.iloc[:, -1]\n",
    "\n",
    "# Columnas\n",
    "train_fma.columns = ['train_fma']\n",
    "validation_fma.columns = ['validation_fma']\n",
    "test_fma.columns = ['test_fma']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_fma.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.tensorflow.org/guide/keras/train_and_evaluate#automatically_setting_apart_a_validation_holdout_set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Correlation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plt.matshow(train.corr())\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Actualizar Datos**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eliminar Campos\n",
    "#test = test.drop(['temp_avg','temp_min', 'atmos_pres_min', 'wd', 'atmos_pres_max','atmos_pres_avg', 'rh','ceil_hgt', 'visibility', 'FMA'], axis=1) \n",
    "# ceil_hgt # visibility ,'temp_max','temp_min'\n",
    "test = test.drop(['atmos_pres_min', 'atmos_pres_max','ceil_hgt', 'visibility', 'FMA'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eliminar Campos\n",
    "#train = train.drop(['temp_avg','temp_min', 'atmos_pres_min', 'wd', 'atmos_pres_max','atmos_pres_avg', 'rh','ceil_hgt', 'visibility', 'FMA'], axis=1)\n",
    "train = train.drop(['atmos_pres_min', 'atmos_pres_max','ceil_hgt', 'visibility', 'FMA'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eliminar Campos\n",
    "#validation = validation.drop(['temp_avg','temp_min', 'atmos_pres_min', 'wd', 'atmos_pres_max','atmos_pres_avg', 'rh','ceil_hgt', 'visibility', 'FMA'], axis=1)\n",
    "validation = validation.drop(['atmos_pres_min', 'atmos_pres_max','ceil_hgt', 'visibility', 'FMA'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Descriptive statistics for each column\n",
    "#test.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training dataset shape:\", train.shape)\n",
    "print(\"Test dataset shape:\", validation.shape)\n",
    "print(\"Test dataset shape:\", test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plt.matshow(train.corr())\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fig, ax = plt.subplots(figsize=(14, 6), dpi=80)\n",
    "#ax.plot(train['temp_max'], label='temp_max',  color='blue', animated = False,\n",
    "#        linestyle='None', marker='*', linewidth=0.5)\n",
    "#ax.plot(train['temp_avg'], label='temp_avg', color='red', animated = True, linewidth=1)\n",
    "#plt.legend(loc='lower left')\n",
    "#ax.set_title('Testing de lectura de Variables', fontsize=16)\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fig, ax = plt.subplots(figsize=(14, 6), dpi=80)\n",
    "#ax.plot(train['ws'], label='ws',  color='blue', animated = False,\n",
    "#        linestyle='None', marker='.', linewidth=0.5)\n",
    "#ax.plot(train['temp_avg'], label='temp_avg', color='red', animated = True, linewidth=1)\n",
    "#plt.legend(loc='lower left')\n",
    "#ax.set_title('Testing de lectura de Variables', fontsize=16)\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalize the data or Standardize the data ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize the data\n",
    "scaler = MinMaxScaler()\n",
    "X_train = scaler.fit_transform(train)\n",
    "X_test = scaler.transform(test)\n",
    "X_validation = scaler.transform(validation)\n",
    "scaler_filename = \"scaler_data\"\n",
    "joblib.dump(scaler, scaler_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reshape inputs for LSTM [samples, timesteps, features]\n",
    "X_train = X_train.reshape(X_train.shape[0], 1, X_train.shape[1]) # X_train \n",
    "print(\"Training data shape:\", X_train.shape)\n",
    "X_test = X_test.reshape(X_test.shape[0], 1, X_test.shape[1])     # X_test \n",
    "print(\"Test data shape:\", X_test.shape)\n",
    "X_validation = X_validation.reshape(X_validation.shape[0], 1, X_validation.shape[1])  #X_validation\n",
    "print(\"Test data shape:\", X_validation.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the autoencoder network model\n",
    "def autoencoder_model(X):\n",
    "    inputs = Input(shape=(X.shape[1], X.shape[2]))\n",
    "    L1 = LSTM(32, activation='relu', return_sequences=True,  #16\n",
    "              kernel_regularizer=regularizers.l2(0.00))(inputs)\n",
    "    L2 = LSTM(4, activation='relu', return_sequences=False)(L1)\n",
    "    L3 = RepeatVector(X.shape[1])(L2)\n",
    "    L4 = LSTM(4, activation='relu', return_sequences=True)(L3)\n",
    "    L5 = LSTM(32, activation='relu', return_sequences=True)(L4) #16\n",
    "    output = TimeDistributed(Dense(X.shape[2]))(L5)    \n",
    "    model = Model(inputs=inputs, outputs=output)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the autoencoder model\n",
    "model = autoencoder_model(X_train)\n",
    "model.compile(optimizer='adamax', loss='mae', metrics=['mae', 'mse', 'mape', 'msle', \n",
    "                                                     'cosine_proximity']) #Error absoluto medio (mae), MeanSquaredError mse\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# collect data across multiple repeats\n",
    "dtrain = DataFrame()\n",
    "val = DataFrame()\n",
    "# fit the model to the data\n",
    "for i in range(2):\n",
    "    print(\"Fit model on training data...\")\n",
    "    start = time.time()\n",
    "    nb_epochs = 200 # 100\n",
    "    batch_size = 32 # 10 # 32 casi por defecto...\n",
    "    \n",
    "    history = model.fit(X_train, X_train, epochs=nb_epochs, batch_size=batch_size,\n",
    "                        validation_data=(X_validation, X_validation), verbose= 0).history # we use 5% of the training data for validation \n",
    "                                                                  # after each epoch (validation_split = 0.05)    # validation_data=X_validation\n",
    "                        # validation_data=(X_validation, X_validation)\n",
    "    end = time.time()\n",
    "    # story history\n",
    "    dtrain[str(i)] = history['loss']\n",
    "    val[str(i)] = history['val_loss']\n",
    "    print(\"Time to training model: \", end='')\n",
    "    print(end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grafo que evalua las distintas iteraciones\n",
    "#print(dtrain.describe())\n",
    "#dtrain.boxplot()\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the best MSE reached on the test set\n",
    "print(\"Best MSE on Validation Set =\", max(history['val_mae']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(\"#########################################################\")\n",
    "loss = model.evaluate(X_train, X_train, verbose=0)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(model.metrics_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = model.evaluate(X_test, X_test, verbose=0)\n",
    "#print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print('loss', loss[0])\n",
    "#print('mae', loss[1])\n",
    "#print('mse', loss[2])\n",
    "#print('mape', loss[3])\n",
    "#print('msle', loss[4])\n",
    "#print('cosine_proximity', loss[5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mejor !!!\n",
    "for name, value in zip(model.metrics_names, loss):\n",
    "    print(name, value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the training losses\n",
    "fig, ax = plt.subplots(figsize=(14, 6), dpi=300)\n",
    "ax.plot(history['loss'], 'b', label='Train', linewidth=2)\n",
    "ax.plot(history['val_loss'], 'r', label='Validation', linewidth=2)\n",
    "ax.set_title('Model loss', fontsize=16)\n",
    "ax.set_ylabel('Loss (mae)')\n",
    "ax.set_xlabel('Epoch')\n",
    "ax.legend(loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the training losses\n",
    "fig, ax = plt.subplots(figsize=(14, 6), dpi=80)\n",
    "ax.plot(history['mse'], 'r', label='Train', linewidth=2)\n",
    "ax.plot(history['val_mse'], 'g', label='Validation', linewidth=2)\n",
    "ax.set_title('Training Mean Squared Error vs Validation Mean Squared Error', fontsize=16)\n",
    "ax.set_ylabel('mse')\n",
    "ax.set_xlabel('Epoch')\n",
    "ax.legend(loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot metrics\n",
    "plt.plot(history['mse'],'r--,', label='mse')\n",
    "plt.plot(history['mae'],'g', label='mae')\n",
    "# plt.plot(history['mape'],'y', label='mape')\n",
    "# plt.plot(history['cosine_proximity'], 'b', label='cosine_proximity')\n",
    "plt.legend(loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distribution of Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the loss distribution of the training set\n",
    "X_pred = model.predict(X_train)\n",
    "X_pred = X_pred.reshape(X_pred.shape[0], X_pred.shape[2])\n",
    "X_pred = pd.DataFrame(X_pred, columns=train.columns)\n",
    "X_pred.index = train.index\n",
    "\n",
    "scored = pd.DataFrame(index=train.index)\n",
    "Xtrain = X_train.reshape(X_train.shape[0], X_train.shape[2])\n",
    "scored['Loss_mae'] = np.mean(np.abs(X_pred-Xtrain), axis = 1)\n",
    "plt.figure(figsize=(16,9), dpi= 300)\n",
    "plt.title('Loss Distribution', fontsize=16)\n",
    "# Add a vertical line, here we set the style in the function call\n",
    "plt.axvline(0.06, ls='--', color='r')\n",
    "plt.annotate('Threshold = 0.06',\n",
    "            xy=(0.06, 30), xycoords='data', fontsize=14,\n",
    "            horizontalalignment='center', verticalalignment='bottom')\n",
    "sns.distplot(scored['Loss_mae'], bins = 40, kde= True, color = 'blue');\n",
    "plt.xlim([0.0,.4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the loss on the test set\n",
    "X_pred = model.predict(X_test)\n",
    "X_pred = X_pred.reshape(X_pred.shape[0], X_pred.shape[2])\n",
    "X_pred = pd.DataFrame(X_pred, columns=test.columns)\n",
    "X_pred.index = test.index\n",
    "\n",
    "scored = pd.DataFrame(index=test.index)\n",
    "Xtest = X_test.reshape(X_test.shape[0], X_test.shape[2])\n",
    "scored['Loss_mae'] = np.mean(np.abs(X_pred-Xtest), axis = 1)\n",
    "scored['Threshold'] = 0.025 # 0.275\n",
    "scored['Anomaly'] = scored['Loss_mae'] > scored['Threshold']\n",
    "#scored.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scored.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#scored.to_csv('FMA.csv', sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_filter = scored[scored['Anomaly'] == True]\n",
    "#data_filter.head(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data_filter.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guardar los datos los FMA\n",
    "#data_filter.to_csv('FMA.csv', sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the same metrics for the training set \n",
    "# and merge all data in a single dataframe for plotting\n",
    "X_pred_train = model.predict(X_train)\n",
    "X_pred_train = X_pred_train.reshape(X_pred_train.shape[0], X_pred_train.shape[2])\n",
    "X_pred_train = pd.DataFrame(X_pred_train, columns=train.columns)\n",
    "X_pred_train.index = train.index\n",
    "\n",
    "scored_train = pd.DataFrame(index=train.index)\n",
    "scored_train['Loss_mae'] = np.mean(np.abs(X_pred_train-Xtrain), axis = 1)\n",
    "scored_train['Threshold'] = 0.025 #0.275\n",
    "scored_train['Anomaly'] = scored_train['Loss_mae'] > scored_train['Threshold']\n",
    "scored = pd.concat([scored_train, scored])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having calculated the loss distribution and the anomaly threshold, we can visualize the model output in the time leading up to the bearing failure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot bearing failure time plot\n",
    "scored.plot(logy=True,  figsize=(16,9), ylim=[1e-2,1e2], color=['blue','red'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Determinación de threshold con (max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get train MAE loss.\n",
    "x_train_pred = model.predict(X_train)\n",
    "train_mae_loss = np.mean(np.abs(x_train_pred - X_train), axis=1)\n",
    "plt.figure(figsize=(16,9), dpi= 300)\n",
    "plt.hist(train_mae_loss, bins= 40)\n",
    "plt.xlabel(\"Train MAE loss\")\n",
    "plt.ylabel(\"No of samples\")\n",
    "plt.xlim([0.0,.25])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get reconstruction loss threshold.\n",
    "threshold = np.max(train_mae_loss)\n",
    "print(\"Reconstruction error threshold: \", threshold)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(\"TF version:\" + tf.__version__)\n",
    "#print(tf.config.list_physical_devices('GPU'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Determinación de threshold con (mean + std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forma tonta de hacer lo mismo\n",
    "reconstructions = model.predict(X_train)\n",
    "train_loss = np.mean(np.abs(reconstructions - X_train), axis=1)\n",
    "threshold = np.mean(train_loss) + np.std(train_loss)\n",
    "print(\"Threshold: \", threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_stats(ytest, ypred):\n",
    "    print(\"Accuracy: {:.5f}, Cohen's Kappa Score: {:.5f}\".format(\n",
    "        accuracy_score(ytest, ypred), \n",
    "        cohen_kappa_score(ytest, ypred, weights=\"quadratic\")))\n",
    "    ll = log_loss(ytest, ypred)\n",
    "    print(\"Log Loss: {}\".format(ll))\n",
    "    print(' ')\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(confusion_matrix(ytest, ypred))\n",
    "    print(\"Classification Report:\")\n",
    "    print(classification_report(ytest, ypred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Version Dago .02\n",
    "## Determinación de Clasificación. Test\n",
    "## Análisis más aceptado!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold_fixed = 0.01 #0.1\n",
    "# calculate the loss on the test set\n",
    "X_pred = model.predict(X_test)\n",
    "X_pred = X_pred.reshape(X_pred.shape[0], X_pred.shape[2])\n",
    "X_pred = pd.DataFrame(X_pred, columns=test.columns)\n",
    "X_pred.index = test.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "error_df = pd.DataFrame(index=test.index)\n",
    "Xtest = X_test.reshape(X_test.shape[0], X_test.shape[2])\n",
    "error_df['Reconstruction_error'] = np.mean(np.power(X_pred-Xtest, 2), axis = 1)\n",
    "error_df['True_class'] = test_fma\n",
    "error_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision_rt, recall_rt, threshold_rt = precision_recall_curve(error_df.True_class, error_df.Reconstruction_error)\n",
    "plt.plot(threshold_rt, precision_rt[1:], label=\"Precision\",linewidth=2)\n",
    "plt.plot(threshold_rt, recall_rt[1:], label=\"Recall\",linewidth=2)\n",
    "plt.title('Precision and recall for different threshold values')\n",
    "plt.xlabel('Threshold')\n",
    "plt.ylabel('Precision/Recall')\n",
    "#plt.xlim([0.0,0.2]) # Permite gestionar la figura\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(threshold_rt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ROC Curve Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "false_pos_rate, true_pos_rate, thresholds = roc_curve(error_df.True_class, error_df.Reconstruction_error)\n",
    "roc_auc = auc(false_pos_rate, true_pos_rate,)\n",
    "\n",
    "plt.plot(false_pos_rate, true_pos_rate, linewidth = 1, label='AUC = %0.3f'% roc_auc)\n",
    "plt.plot([0,1],[0,1], linewidth = 3)\n",
    "\n",
    "plt.xlim([-0.01, 1])\n",
    "plt.ylim([0, 1.01])\n",
    "plt.legend(loc='lower right')\n",
    "plt.title('Receiver operating characteristic curve (ROC)')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.xlabel('False Positive Rate')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recall vs. Precision Thresholding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision_rt, recall_rt, threshold_rt = precision_recall_curve(error_df.True_class, error_df.Reconstruction_error)\n",
    "plt.plot(recall_rt, precision_rt, linewidth = 1, label='Precision-Recall curve')\n",
    "plt.legend(loc='upper right')\n",
    "plt.title('Recall vs Precision')\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reconstruction Error vs Threshold Check. Importante threshold_fixed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#threshold_fixed = 0.07 #0.1\n",
    "groups = error_df.groupby('True_class')\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "for name, group in groups:\n",
    "    ax.plot(group.index, group.Reconstruction_error, marker='o', ms=3.5, linestyle='',\n",
    "            label= \"FMA\" if name == 1 else \"Normal\")\n",
    "ax.hlines(threshold_fixed, ax.get_xlim()[0], ax.get_xlim()[1], colors=\"r\", zorder=100, label='Threshold')\n",
    "ax.legend()\n",
    "plt.title(\"Reconstruction error for different classes\")\n",
    "plt.ylabel(\"Reconstruction error\")\n",
    "plt.xlabel(\"Data point index\")\n",
    "plt.show(); "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_y = [1 if e > threshold_fixed else 0 for e in error_df.Reconstruction_error.values]\n",
    "conf_matrix = confusion_matrix(error_df.True_class, pred_y)\n",
    "\n",
    "plt.figure(figsize=(12, 12))\n",
    "sns.heatmap(conf_matrix, xticklabels=LABELS, yticklabels=LABELS, annot=True, fmt=\"d\");\n",
    "plt.title(\"Confusion matrix\")\n",
    "plt.ylabel('True class')\n",
    "plt.xlabel('Predicted class')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forma tonta de hacer lo mismo\n",
    "reconstructions = model.predict(X_test)\n",
    "test_loss = np.mean(np.abs(reconstructions - X_test), axis=1)\n",
    "threshold = np.mean(test_loss) + np.std(test_loss)\n",
    "print(\"Threshold: \", threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# error_df.True_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_stats(error_df.True_class, pred_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = precision_score(error_df.True_class, pred_y)\n",
    "r = recall_score(error_df.True_class, pred_y)\n",
    "f = fbeta_score(error_df.True_class, pred_y, beta= 0.5)\n",
    "print('Result: p=%.3f, r=%.3f, f=%.3f' % (p, r, f))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reconstruction error - Test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plt.figure(figsize=(10,6))\n",
    "sns.kdeplot(error_df.Reconstruction_error[error_df.True_class==0], label='Normal', shade=True, clip=(0,10))\n",
    "sns.kdeplot(error_df.Reconstruction_error[error_df.True_class==1], label='FMA', shade=True, clip=(0,10))\n",
    "# Add a vertical line, here we set the style in the function call\n",
    "#plt.axvline(0.01, ls='--', color='r')\n",
    "#plt.annotate('Threshold = 0.01',\n",
    "#            xy=(0.01, 250), xycoords='data', fontsize=14,\n",
    "#            horizontalalignment='center', verticalalignment='bottom')\n",
    "plt.xlabel('Reconstruction error');\n",
    "plt.title('Reconstruction error - Test set')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_y = [1 if e > threshold_fixed else 0 for e in error_df.Reconstruction_error.values]\n",
    "conf_matrix = confusion_matrix(error_df.True_class, pred_y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# threshold\n",
    "for threshold_fixed in [0.01, 0.02, 0.04, 0.05]:\n",
    "    pred_y = [1 if e > threshold_fixed else 0 for e in error_df.Reconstruction_error.values]\n",
    "    conf_matrix = confusion_matrix(error_df.True_class, pred_y)\n",
    "    print(conf_matrix)\n",
    "    p = precision_score(error_df.True_class, pred_y)\n",
    "    r = recall_score(error_df.True_class, pred_y)\n",
    "    f = fbeta_score(error_df.True_class, pred_y, beta= 0.5)\n",
    "    print('Result: p=%.3f, r=%.3f, f=%.3f' % (p, r, f))\n",
    "    print_stats(error_df.True_class, pred_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}