{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python38564bit129eeb57289c44939b4863da13de07dc",
   "display_name": "Python 3.8.5 64-bit",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pd.DataFrame({\"deceptive\": \"truthful\", \"text\": \"This hotel has been great! We recommend it a lot.\"}, index = [1600])\n",
    "df = df.append(df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "      deceptive             hotel  polarity       source  \\\n0      truthful            conrad  positive  TripAdvisor   \n1      truthful             hyatt  positive  TripAdvisor   \n2      truthful             hyatt  positive  TripAdvisor   \n3      truthful              omni  positive  TripAdvisor   \n4      truthful             hyatt  positive  TripAdvisor   \n...         ...               ...       ...          ...   \n1595  deceptive  intercontinental  negative        MTurk   \n1596  deceptive            amalfi  negative        MTurk   \n1597  deceptive  intercontinental  negative        MTurk   \n1598  deceptive            palmer  negative        MTurk   \n1599  deceptive            amalfi  negative        MTurk   \n\n                                                   text  \n0     We stayed for a one night getaway with family ...  \n1     Triple A rate with upgrade to view room was le...  \n2     This comes a little late as I'm finally catchi...  \n3     The Omni Chicago really delivers on all fronts...  \n4     I asked for a high floor away from the elevato...  \n...                                                 ...  \n1595  Problems started when I booked the InterContin...  \n1596  The Amalfi Hotel has a beautiful website and i...  \n1597  The Intercontinental Chicago Magnificent Mile ...  \n1598  The Palmer House Hilton, while it looks good i...  \n1599  As a former Chicagoan, I'm appalled at the Ama...  \n\n[1600 rows x 5 columns]\n"
     ]
    }
   ],
   "source": [
    "## Load data from a csv file, pre-process the content of it and then generate the training and test dataset\n",
    "## Cargar datos desde un fichero csv, preprocesar sus contenidos y generar el dataset de entreno y testing\n",
    "import pandas as pd\n",
    "from pandas import DataFrame\n",
    "\n",
    "df = pd.read_csv('/home/zan/Downloads/deceptive-opinion.csv')\n",
    "\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "  deceptive   hotel  polarity       source  \\\n",
       "0  truthful  conrad  positive  TripAdvisor   \n",
       "1  truthful   hyatt  positive  TripAdvisor   \n",
       "2  truthful   hyatt  positive  TripAdvisor   \n",
       "3  truthful    omni  positive  TripAdvisor   \n",
       "4  truthful   hyatt  positive  TripAdvisor   \n",
       "\n",
       "                                                text  \n",
       "0  We stayed for a one night getaway with family ...  \n",
       "1  Triple A rate with upgrade to view room was le...  \n",
       "2  This comes a little late as I'm finally catchi...  \n",
       "3  The Omni Chicago really delivers on all fronts...  \n",
       "4  I asked for a high floor away from the elevato...  "
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>deceptive</th>\n      <th>hotel</th>\n      <th>polarity</th>\n      <th>source</th>\n      <th>text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>truthful</td>\n      <td>conrad</td>\n      <td>positive</td>\n      <td>TripAdvisor</td>\n      <td>We stayed for a one night getaway with family ...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>truthful</td>\n      <td>hyatt</td>\n      <td>positive</td>\n      <td>TripAdvisor</td>\n      <td>Triple A rate with upgrade to view room was le...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>truthful</td>\n      <td>hyatt</td>\n      <td>positive</td>\n      <td>TripAdvisor</td>\n      <td>This comes a little late as I'm finally catchi...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>truthful</td>\n      <td>omni</td>\n      <td>positive</td>\n      <td>TripAdvisor</td>\n      <td>The Omni Chicago really delivers on all fronts...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>truthful</td>\n      <td>hyatt</td>\n      <td>positive</td>\n      <td>TripAdvisor</td>\n      <td>I asked for a high floor away from the elevato...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 367
    }
   ],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Replace missing values & eliminate duplicated values/etc.\n",
    "## Sustituir valores nulos/missing y eliminar valores duplicados \n",
    "## 1.Data Cleaning: missing data, noisy data\n",
    "missing_values = [\"n/a\", \"na\", \"--\"]\n",
    "df.isnull().sum() ## No hay valores de tipo missing\n",
    "df[df.duplicated(keep=False)] ## 803-853, 847-862,  995-1014, 1085-1109\n",
    "df = df.drop_duplicates() ## Hay cuatro duplicados en el dataset\n",
    "## Eliminar datos que no nos interesan para nuestro entrenamiento\n",
    "## df = df.drop(df.columns[[1, 2, 3]], axis = 1)\n",
    "\n",
    "\n",
    "#df.iloc[803] == df.iloc[853]\n",
    "#df.iloc[803].equals(df.iloc[853])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "  deceptive                                               text\n",
       "0  truthful  We stayed for a one night getaway with family ...\n",
       "1  truthful  Triple A rate with upgrade to view room was le...\n",
       "2  truthful  This comes a little late as I'm finally catchi...\n",
       "3  truthful  The Omni Chicago really delivers on all fronts...\n",
       "4  truthful  I asked for a high floor away from the elevato..."
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>deceptive</th>\n      <th>text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>truthful</td>\n      <td>We stayed for a one night getaway with family ...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>truthful</td>\n      <td>Triple A rate with upgrade to view room was le...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>truthful</td>\n      <td>This comes a little late as I'm finally catchi...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>truthful</td>\n      <td>The Omni Chicago really delivers on all fronts...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>truthful</td>\n      <td>I asked for a high floor away from the elevato...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 369
    }
   ],
   "source": [
    "## 2.Data transformation: normalization, attribute selection, discretization, hierarchy generation\n",
    "\n",
    "## Normalizacion de los datos dentro de un cierto intervalo: no es necesario al no tener campos con valores continuos\n",
    "## Seleccionamos solo aquellas caracteristicas que nos interesen\n",
    "## Eliminate columns -- 1-3 as they won't contribute to the model\n",
    "## Eliminar columnas del 1 al 3 ya que no aportan informacion adiccional al modelo\n",
    "\n",
    "df = df.drop(df.columns[[1, 2, 3]], axis = 1)\n",
    "df.head(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "796\n800\n"
     ]
    }
   ],
   "source": [
    "## The deceptive and truthful registers are balanced: 800 - 800\n",
    "## La clase deceptive esta balaneceada, tiene 800 registros deceptive y 800 registros truthful\n",
    "print(len(df[df['deceptive'] == 'truthful']))\n",
    "print(len(df[df['deceptive'] == 'deceptive']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 3.Data Reduction: Data Cube aggregation, attribute subset selection, numerosity reduction, dimensionality reduction\n",
    "\n",
    "## Esta parte nos interesaria para modificar y convertir las opiniones de texto a un vectores de pesos(int) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "\"The Palmer House Hilton, while it looks good in pictures, and the outside, is actually a disaster of a hotel. When I went through, the lobby was dirty, my room hadn't been cleaned, and smelled thoroughly of smoke. When I requested more pillows, the lady on the phone scoffed at me and said she'd send them up. It took over an hour for 2 pillows. This hotel is a good example that what you pay for isn't always what you get. I will not be returning.\\n\""
      ]
     },
     "metadata": {},
     "execution_count": 371
    }
   ],
   "source": [
    "df['text'][1598]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Limpiar texto, opinion del usuario para luego poder convertir la secuencia a vector\n",
    "\n",
    "from gensim import utils\n",
    "import gensim.parsing.preprocessing as gsp\n",
    "\n",
    "filters = [\n",
    "           gsp.strip_tags, \n",
    "           gsp.strip_punctuation,\n",
    "           gsp.strip_multiple_whitespaces,\n",
    "           gsp.strip_numeric,\n",
    "           gsp.strip_short,\n",
    "           #gsp.remove_stopwords, \n",
    "           gsp.stem_text\n",
    "          ]\n",
    "\n",
    "def clean_text(s):\n",
    "    s = s.lower()\n",
    "    s = utils.to_unicode(s)\n",
    "    for f in filters:\n",
    "        s = f(s)\n",
    "    return s\n",
    "\n",
    "df['text'] = df['text'].apply(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[('the', 15965),\n",
       " ('and', 7905),\n",
       " ('wa', 5828),\n",
       " ('hotel', 3642),\n",
       " ('room', 3494),\n",
       " ('for', 2886),\n",
       " ('stai', 2261)]"
      ]
     },
     "metadata": {},
     "execution_count": 373
    }
   ],
   "source": [
    "Counter(\" \".join(df[\"text\"]).split()).most_common(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "for x in Counter(\" \".join(df[\"text\"]).split()).most_common(7):\n",
    "    df['text'] = df['text'].str.replace(x[0], '')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Es posible que no sea necesario quitar los stopwords porque convirtiria: I will not be returning(negativo). en --> return(positivo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "' palmer hous hilton while look good pictur   outsid actual disast  when went through  lobbi  dirti  hadn been clean  smell thoroughli smoke when request more pillow  ladi  phone scof  said she send m took over hour  pillow thi  good exampl that what you pai  isn ali what you get will not return'"
      ]
     },
     "metadata": {},
     "execution_count": 375
    }
   ],
   "source": [
    "df['text'][1598]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Generate training and testing dataset\n",
    "training_split = 0.75\n",
    "testing_split = 0.25\n",
    "pos_testing_split = 0.9 ## truthful + deceptive, los deceptive entre 5-10% de esta suma.\n",
    "neg_deceptive_split = 0.1\n",
    "\n",
    "deceptive_training = df[df.deceptive == 'truthful'].sample(frac = training_split)\n",
    "newdf = df.drop(deceptive_training.index.values)\n",
    "pos_deceptive_testing = newdf[newdf.deceptive == 'truthful'].sample(frac = pos_testing_split)\n",
    "neg_deceptive_testing = newdf[newdf.deceptive == 'deceptive'].sample(int(neg_deceptive_split * (testing_split * len(df[df.deceptive == 'truthful']))))\n",
    "\n",
    "frames = [pos_deceptive_testing, neg_deceptive_testing]\n",
    "deceptive_testing = pd.concat(frames)\n",
    "deceptive_testing = deceptive_testing.sample(frac = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0    [on, night, getai, with, famili, thursdai, tri...\n1    [tripl, rate, with, upgrad, view, less, than, ...\n2    [thi, come, littl, late, final, catch, review,...\n3    [omni, chicago, realli, deliv, all, front, fro...\n4    [ask, high, floor, ai, from, elev, that, what,...\n5    [omni, on, night, follow, busi, meet, anoth, d...\n6    [conrad, night, just, be, thanksgiv, had, corn...\n7    [just, got, back, from, dai, chicago, shop, wi...\n8    [arriv, omni, septemb, dai, took, ill, when, l...\n9    [our, visit, chicago, chose, hyatt, due, it, l...\nName: text, dtype: object\n"
     ]
    }
   ],
   "source": [
    "from gensim.utils import simple_preprocess\n",
    "df['text'] = [simple_preprocess(line, deacc=True) for line in df['text']] \n",
    "print(df['text'].head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "metadata": {},
   "outputs": [],
   "source": [
    "## NLTK tokenizer\n",
    "\n",
    "import nltk\n",
    "import gensim\n",
    "from nltk.corpus import stopwords\n",
    "from gensim.models import Doc2Vec\n",
    "from gensim.models.doc2vec import TaggedDocument\n",
    "def tokenize_text(text):\n",
    "    tokens = []\n",
    "    for sent in nltk.sent_tokenize(text):\n",
    "        for word in nltk.word_tokenize(sent):\n",
    "            if len(word) < 2: ## solo las palabras con longitud > 2 se tokenizan\n",
    "                continue\n",
    "            tokens.append(word.lower())\n",
    "    return tokens\n",
    "train_dec = deceptive_training.apply(\n",
    "    lambda x: TaggedDocument(words=tokenize_text(x['text']), tags=[x.deceptive]), axis=1)\n",
    "test_dec = deceptive_testing.apply(\n",
    "    lambda x: TaggedDocument(words=tokenize_text(x['text']), tags=[x.deceptive]), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "TaggedDocument(words=['fianc', 'travel', 'chicago', 'first', 'time', 'thi', 'decemb', 'were', 'not', 'happi', 'with', 'thi', 'friendliest', 'peopl', 'staff', 'were', 'bell', 'boi', 'greet', 'with', 'smile', 'gave', 'lot', 'great', 'advic', 'throughout', 'our', 'rest', 'staff', 'howev', 'terribl', 'when', 'first', 'check', 'instantli', 'discourag', 'front', 'desk', 'person', 'not', 'good', 'mood', 'couldn', 'it', 'leav', 'lobbi', 'have', 'not', 'travel', 'here', 'be', 'were', 'look', 'excit', 'place', 'eat', 'sightse', 'ask', 'multipl', 'staff', 'member', 'front', 'desk', 'place', 'see', 'never', 'got', 'ani', 'good', 'recommend', 'felt', 'were', 'inconveni', 'staff', 'on', 'point', 'front', 'desk', 'person', 'phone', 'while', 'were', 'it', 'talk', 'her', 'she', 'phone', 'famili', 'four', 'came', 'behind', 'it', 'talk', 'her', 'well', 'even', 'though', 'she', 'saw', 'st', 're', 'after', 'she', 'finish', 'with', 'phone', 'call', 'she', 'ignor', 'help', 'famili', 'first', 'ye', 'ar', 'our', 'mid', 'but', 'ar', 'still', 'pai', 'guest', 're', 'reason', 'treat', 'like', 'ar', 'invis', 'how', 'much', 'monei', 'paid', 'each', 'night', 'would', 'have', 'rar', 'somewher', 'els', 'decid', 'with', 'thi', 'becaus', 'locat', 'great', 'review', 'but', 'did', 'not', 'experi', 'ani', 'thi', 'dure', 'our', 'were', 'alright', 'but', 'noth', 'special', 'would', 'not', 'thi', 'again'], tags=['truthful'])"
      ]
     },
     "metadata": {},
     "execution_count": 379
    }
   ],
   "source": [
    "train_dec[958]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "AttributeError",
     "evalue": "'Doc2VecTrainables' object has no attribute 'vectors_lockf'",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-380-523044218c66>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mdbow\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDoc2Vec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvector_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m200\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnegative\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwindow\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mworkers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcores\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mvec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdbow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfer_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Top 10 values in Doc2Vec inferred vector:\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/gensim/models/doc2vec.py\u001b[0m in \u001b[0;36minfer_vector\u001b[0;34m(self, doc_words, alpha, min_alpha, epochs, steps)\u001b[0m\n\u001b[1;32m    686\u001b[0m                 )\n\u001b[1;32m    687\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 688\u001b[0;31m                 train_document_dm(\n\u001b[0m\u001b[1;32m    689\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdoc_words\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdoctag_indexes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwork\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mneu1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    690\u001b[0m                     \u001b[0mlearn_words\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearn_hidden\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdoctag_vectors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdoctag_vectors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdoctag_locks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdoctag_locks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/gensim/models/doc2vec_inner.pyx\u001b[0m in \u001b[0;36mgensim.models.doc2vec_inner.train_document_dm\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/gensim/models/doc2vec_inner.pyx\u001b[0m in \u001b[0;36mgensim.models.doc2vec_inner.init_d2v_config\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Doc2VecTrainables' object has no attribute 'vectors_lockf'"
     ]
    }
   ],
   "source": [
    "dbow = Doc2Vec(dm=1, vector_size = 200, negative=5, window = 3, min_count = 2, sample = 0, workers = cores)\n",
    "vec = dbow.infer_vector(df['text'][50])\n",
    "print(df['text'][50])\n",
    "print(len(vec))\n",
    "print(\"Top 10 values in Doc2Vec inferred vector:\")\n",
    "print(vec[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 597/597 [00:00<00:00, 1130984.41it/s]\n"
     ]
    }
   ],
   "source": [
    "## Build vocabulary using distributed bag of words\n",
    "from tqdm import tqdm\n",
    "import multiprocessing\n",
    "cores = multiprocessing.cpu_count()\n",
    "dbow = Doc2Vec(dm=1, vector_size = 200, negative=5, window = 3, min_count = 2, sample = 0, workers = cores)\n",
    "dbow.build_vocab([x for x in tqdm(train_dec.values)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 597/597 [00:00<00:00, 1369053.85it/s]\n",
      "100%|██████████| 597/597 [00:00<00:00, 1313056.89it/s]\n",
      "100%|██████████| 597/597 [00:00<00:00, 1802735.41it/s]\n",
      "100%|██████████| 597/597 [00:00<00:00, 901367.71it/s]\n",
      "100%|██████████| 597/597 [00:00<00:00, 1601022.69it/s]\n",
      "100%|██████████| 597/597 [00:00<00:00, 1657180.34it/s]\n",
      "100%|██████████| 597/597 [00:00<00:00, 1313745.80it/s]\n",
      "100%|██████████| 597/597 [00:00<00:00, 1574842.45it/s]\n",
      "100%|██████████| 597/597 [00:00<00:00, 1932098.37it/s]\n",
      "100%|██████████| 597/597 [00:00<00:00, 1071000.64it/s]\n",
      "100%|██████████| 597/597 [00:00<00:00, 1620711.64it/s]\n",
      "100%|██████████| 597/597 [00:00<00:00, 1329086.78it/s]\n",
      "100%|██████████| 597/597 [00:00<00:00, 830789.48it/s]\n",
      "100%|██████████| 597/597 [00:00<00:00, 1132518.99it/s]\n",
      "100%|██████████| 597/597 [00:00<00:00, 1521263.36it/s]\n",
      "100%|██████████| 597/597 [00:00<00:00, 1590851.01it/s]\n",
      "100%|██████████| 597/597 [00:00<00:00, 1815808.19it/s]\n",
      "100%|██████████| 597/597 [00:00<00:00, 2127442.22it/s]\n",
      "100%|██████████| 597/597 [00:00<00:00, 1954722.47it/s]\n",
      "100%|██████████| 597/597 [00:00<00:00, 1492252.38it/s]\n",
      "100%|██████████| 597/597 [00:00<00:00, 574575.38it/s]\n",
      "100%|██████████| 597/597 [00:00<00:00, 1175034.95it/s]\n",
      "100%|██████████| 597/597 [00:00<00:00, 1411499.15it/s]\n",
      "100%|██████████| 597/597 [00:00<00:00, 1348411.14it/s]\n",
      "100%|██████████| 597/597 [00:00<00:00, 950284.44it/s]\n",
      "100%|██████████| 597/597 [00:00<00:00, 1758426.61it/s]\n",
      "100%|██████████| 597/597 [00:00<00:00, 1379614.04it/s]\n",
      "100%|██████████| 597/597 [00:00<00:00, 1571876.64it/s]\n",
      "100%|██████████| 597/597 [00:00<00:00, 1238990.35it/s]\n",
      "100%|██████████| 597/597 [00:00<00:00, 775712.36it/s]\n",
      "CPU times: user 8.35 s, sys: 672 ms, total: 9.03 s\n",
      "Wall time: 5.04 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from sklearn import utils\n",
    "for epoch in range(30):\n",
    "    dbow.train(utils.shuffle([x for x in tqdm(train_dec.values)]), total_examples=len(train_dec.values), epochs=1)\n",
    "    dbow.alpha -= 0.002\n",
    "    dbow.min_alpha = dbow.alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vec_for_learning(model, tagged_docs):\n",
    "    sents = tagged_docs.values\n",
    "    targets, regressors = zip(*[(doc.tags[0], model.infer_vector(doc.words, steps=20)) for doc in sents])\n",
    "    return targets, regressors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train, x_train = vec_for_learning(dbow, train_dec)\n",
    "y_test, x_test = vec_for_learning(dbow, test_dec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[('where', 0.9833047986030579),\n",
       " ('ton', 0.9828348159790039),\n",
       " ('star', 0.9827905893325806),\n",
       " ('or', 0.981451153755188),\n",
       " ('thank', 0.9809910655021667),\n",
       " ('mean', 0.9799057841300964),\n",
       " ('expect', 0.979249119758606),\n",
       " ('workout', 0.9789624214172363),\n",
       " ('regardless', 0.9786323308944702),\n",
       " ('requir', 0.9778539538383484)]"
      ]
     },
     "metadata": {},
     "execution_count": 389
    }
   ],
   "source": [
    "dbow.most_similar('eat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "metadata": {},
     "execution_count": 158
    }
   ],
   "source": [
    "len(x_train[597])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "pandas.core.series.Series"
      ]
     },
     "metadata": {},
     "execution_count": 144
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "training_list = []\n",
    "for tag, text_val in zip(y_train, x_train):\n",
    "    listToStr = ' '.join([str(elem) for elem in text_val]) \n",
    "    training_list.append([tag,listToStr])\n",
    "\n",
    "testing_list = []\n",
    "for tag, text_val in zip(y_test, x_test):\n",
    "    listToStr = ' '.join([str(elem) for elem in text_val]) \n",
    "    testing_list.append([tag,listToStr])\n",
    "\n",
    "deceptive_training = pd.DataFrame(data=training_list, columns=[\"deceptive\",\"text\"])\n",
    "deceptive_testing = pd.DataFrame(data=testing_list, columns=[\"deceptive\",\"text\"])\n",
    "type(deceptive_training['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "deceptive_training.to_csv(r'/home/zan/Desktop/dl_autoencoder/docs/autoencoder/deceptive_opinion_autoencoder/deceptive_training.csv', index=False, sep=',')\n",
    "deceptive_testing.to_csv(r'/home/zan/Desktop/dl_autoencoder/docs/autoencoder/deceptive_opinion_autoencoder/deceptive_testing.csv', index=False, sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}