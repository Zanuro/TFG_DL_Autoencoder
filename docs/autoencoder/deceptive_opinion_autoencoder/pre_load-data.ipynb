{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python38564bit129eeb57289c44939b4863da13de07dc",
   "display_name": "Python 3.8.5 64-bit",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "      deceptive             hotel  polarity       source  \\\n0      truthful            conrad  positive  TripAdvisor   \n1      truthful             hyatt  positive  TripAdvisor   \n2      truthful             hyatt  positive  TripAdvisor   \n3      truthful              omni  positive  TripAdvisor   \n4      truthful             hyatt  positive  TripAdvisor   \n...         ...               ...       ...          ...   \n1595  deceptive  intercontinental  negative        MTurk   \n1596  deceptive            amalfi  negative        MTurk   \n1597  deceptive  intercontinental  negative        MTurk   \n1598  deceptive            palmer  negative        MTurk   \n1599  deceptive            amalfi  negative        MTurk   \n\n                                                   text  \n0     We stayed for a one night getaway with family ...  \n1     Triple A rate with upgrade to view room was le...  \n2     This comes a little late as I'm finally catchi...  \n3     The Omni Chicago really delivers on all fronts...  \n4     I asked for a high floor away from the elevato...  \n...                                                 ...  \n1595  Problems started when I booked the InterContin...  \n1596  The Amalfi Hotel has a beautiful website and i...  \n1597  The Intercontinental Chicago Magnificent Mile ...  \n1598  The Palmer House Hilton, while it looks good i...  \n1599  As a former Chicagoan, I'm appalled at the Ama...  \n\n[1600 rows x 5 columns]\n"
     ]
    }
   ],
   "source": [
    "## Load data from a csv file, pre-process the content of it and then generate the training and test dataset\n",
    "## Cargar datos desde un fichero csv, preprocesar sus contenidos y generar el dataset de entreno y testing\n",
    "import pandas as pd\n",
    "from pandas import DataFrame\n",
    "\n",
    "df = pd.read_csv('/home/zan/Downloads/deceptive-opinion.csv')\n",
    "\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "  deceptive   hotel  polarity       source  \\\n",
       "0  truthful  conrad  positive  TripAdvisor   \n",
       "1  truthful   hyatt  positive  TripAdvisor   \n",
       "2  truthful   hyatt  positive  TripAdvisor   \n",
       "3  truthful    omni  positive  TripAdvisor   \n",
       "4  truthful   hyatt  positive  TripAdvisor   \n",
       "\n",
       "                                                text  \n",
       "0  We stayed for a one night getaway with family ...  \n",
       "1  Triple A rate with upgrade to view room was le...  \n",
       "2  This comes a little late as I'm finally catchi...  \n",
       "3  The Omni Chicago really delivers on all fronts...  \n",
       "4  I asked for a high floor away from the elevato...  "
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>deceptive</th>\n      <th>hotel</th>\n      <th>polarity</th>\n      <th>source</th>\n      <th>text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>truthful</td>\n      <td>conrad</td>\n      <td>positive</td>\n      <td>TripAdvisor</td>\n      <td>We stayed for a one night getaway with family ...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>truthful</td>\n      <td>hyatt</td>\n      <td>positive</td>\n      <td>TripAdvisor</td>\n      <td>Triple A rate with upgrade to view room was le...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>truthful</td>\n      <td>hyatt</td>\n      <td>positive</td>\n      <td>TripAdvisor</td>\n      <td>This comes a little late as I'm finally catchi...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>truthful</td>\n      <td>omni</td>\n      <td>positive</td>\n      <td>TripAdvisor</td>\n      <td>The Omni Chicago really delivers on all fronts...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>truthful</td>\n      <td>hyatt</td>\n      <td>positive</td>\n      <td>TripAdvisor</td>\n      <td>I asked for a high floor away from the elevato...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 50
    }
   ],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Replace missing values & eliminate duplicated values/etc.\n",
    "## Sustituir valores nulos/missing y eliminar valores duplicados \n",
    "## 1.Data Cleaning: missing data, noisy data\n",
    "missing_values = [\"n/a\", \"na\", \"--\"]\n",
    "df.isnull().sum() ## No hay valores de tipo missing\n",
    "df[df.duplicated(keep=False)] ## 803-853, 847-862,  995-1014, 1085-1109\n",
    "df = df.drop_duplicates() ## Hay cuatro duplicados en el dataset\n",
    "## Eliminar datos que no nos interesan para nuestro entrenamiento\n",
    "## df = df.drop(df.columns[[1, 2, 3]], axis = 1)\n",
    "\n",
    "\n",
    "#df.iloc[803] == df.iloc[853]\n",
    "#df.iloc[803].equals(df.iloc[853])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "  deceptive                                               text\n",
       "0  truthful  We stayed for a one night getaway with family ...\n",
       "1  truthful  Triple A rate with upgrade to view room was le...\n",
       "2  truthful  This comes a little late as I'm finally catchi...\n",
       "3  truthful  The Omni Chicago really delivers on all fronts...\n",
       "4  truthful  I asked for a high floor away from the elevato..."
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>deceptive</th>\n      <th>text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>truthful</td>\n      <td>We stayed for a one night getaway with family ...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>truthful</td>\n      <td>Triple A rate with upgrade to view room was le...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>truthful</td>\n      <td>This comes a little late as I'm finally catchi...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>truthful</td>\n      <td>The Omni Chicago really delivers on all fronts...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>truthful</td>\n      <td>I asked for a high floor away from the elevato...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 52
    }
   ],
   "source": [
    "## Eliminate columns -- 1-3 as they won't contribute to the model\n",
    "## Eliminar columnas del 1 al 3 ya que no aportan informacion adiccional al modelo\n",
    "\n",
    "df = df.drop(df.columns[[1, 2, 3]], axis = 1)\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "796\n800\n"
     ]
    }
   ],
   "source": [
    "## The deceptive and truthful registers are balanced: 800 - 800\n",
    "## La clase deceptive esta balaneceada, tiene 800 registros deceptive y 800 registros truthful\n",
    "print(len(df[df['deceptive'] == 'truthful']))\n",
    "print(len(df[df['deceptive'] == 'deceptive']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 2.Data transformation: normalization, attribute selection, discretization, hierarchy generation\n",
    "\n",
    "## Normalizacion de los datos dentro de un cierto intervalo: no es necesario al no tener campos con valores continuos\n",
    "## Esta parte nos interesaria para modificar y convertir las opiniones de texto a un vectores de pesos(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 3.Data Reduction: Data Cube aggregation, attribute subset selection, numerosity reduction, dimensionality reduction\n",
    "\n",
    "## "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Limpiar texto, opinion del usuario para luego poder convertir la secuencia a vector\n",
    "\n",
    "from gensim import utils\n",
    "import gensim.parsing.preprocessing as gsp\n",
    "\n",
    "filters = [\n",
    "           gsp.strip_tags, \n",
    "           gsp.strip_punctuation,\n",
    "           gsp.strip_multiple_whitespaces,\n",
    "           gsp.strip_numeric,\n",
    "           gsp.remove_stopwords, \n",
    "           gsp.strip_short, \n",
    "           gsp.stem_text\n",
    "          ]\n",
    "\n",
    "def clean_text(s):\n",
    "    s = s.lower()\n",
    "    s = utils.to_unicode(s)\n",
    "    for f in filters:\n",
    "        s = f(s)\n",
    "    return s\n",
    "\n",
    "df['text'] = df['text'].apply(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Generate training and testing dataset\n",
    "training_split = 0.75\n",
    "testing_split = 0.25\n",
    "pos_testing_split = 0.9 ## truthful + deceptive, los deceptive entre 5-10% de esta suma.\n",
    "neg_deceptive_split = 0.1\n",
    "\n",
    "deceptive_training = df[df.deceptive == 'truthful'].sample(frac = training_split)\n",
    "newdf = df.drop(deceptive_training.index.values)\n",
    "pos_deceptive_testing = newdf[newdf.deceptive == 'truthful'].sample(frac = pos_testing_split)\n",
    "neg_deceptive_testing = newdf[newdf.deceptive == 'deceptive'].sample(int(neg_deceptive_split * (testing_split * len(df[df.deceptive == 'truthful']))))\n",
    "\n",
    "frames = [pos_deceptive_testing, neg_deceptive_testing]\n",
    "deceptive_testing = pd.concat(frames)\n",
    "deceptive_testing = deceptive_testing.sample(frac = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "## NLTK tokenizer\n",
    "\n",
    "import nltk\n",
    "import gensim\n",
    "from nltk.corpus import stopwords\n",
    "from gensim.models import Doc2Vec\n",
    "from gensim.models.doc2vec import TaggedDocument\n",
    "def tokenize_text(text):\n",
    "    tokens = []\n",
    "    for sent in nltk.sent_tokenize(text):\n",
    "        for word in nltk.word_tokenize(sent):\n",
    "            if len(word) < 2: ## solo las palabras con longitud > 2\n",
    "                continue\n",
    "            tokens.append(word.lower())\n",
    "    return tokens\n",
    "train_dec = deceptive_training.apply(\n",
    "    lambda x: TaggedDocument(words=tokenize_text(x['text']), tags=[x.deceptive]), axis=1)\n",
    "test_dec = deceptive_testing.apply(\n",
    "    lambda x: TaggedDocument(words=tokenize_text(x['text']), tags=[x.deceptive]), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 597/597 [00:00<00:00, 2047423.95it/s]\n"
     ]
    }
   ],
   "source": [
    "## Build vocabulary using distributed bag of words\n",
    "from tqdm import tqdm\n",
    "import multiprocessing\n",
    "cores = multiprocessing.cpu_count()\n",
    "dbow = Doc2Vec(dm=0, vector_size = 200, negative=5, hs=0, min_count = 2, sample = 0, workers = cores)\n",
    "dbow.build_vocab([x for x in tqdm(train_dec.values)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 597/597 [00:00<00:00, 1899847.87it/s]\n",
      "100%|██████████| 597/597 [00:00<00:00, 1973206.85it/s]\n",
      "100%|██████████| 597/597 [00:00<00:00, 2410009.13it/s]\n",
      "100%|██████████| 597/597 [00:00<00:00, 2539553.23it/s]\n",
      "100%|██████████| 597/597 [00:00<00:00, 1280818.15it/s]\n",
      "100%|██████████| 597/597 [00:00<00:00, 2095397.06it/s]\n",
      "100%|██████████| 597/597 [00:00<00:00, 1802735.41it/s]\n",
      "100%|██████████| 597/597 [00:00<00:00, 1829071.94it/s]\n",
      "100%|██████████| 597/597 [00:00<00:00, 2597509.84it/s]\n",
      "100%|██████████| 597/597 [00:00<00:00, 1877061.09it/s]\n",
      "100%|██████████| 597/597 [00:00<00:00, 1092018.97it/s]\n",
      "100%|██████████| 597/597 [00:00<00:00, 1911449.99it/s]\n",
      "100%|██████████| 597/597 [00:00<00:00, 1726896.20it/s]\n",
      "100%|██████████| 597/597 [00:00<00:00, 1992044.14it/s]\n",
      "100%|██████████| 597/597 [00:00<00:00, 2088406.58it/s]\n",
      "100%|██████████| 597/597 [00:00<00:00, 2213969.49it/s]\n",
      "100%|██████████| 597/597 [00:00<00:00, 1999999.59it/s]\n",
      "100%|██████████| 597/597 [00:00<00:00, 1730476.49it/s]\n",
      "100%|██████████| 597/597 [00:00<00:00, 2125636.24it/s]\n",
      "100%|██████████| 597/597 [00:00<00:00, 1722145.45it/s]\n",
      "100%|██████████| 597/597 [00:00<00:00, 2169843.58it/s]\n",
      "100%|██████████| 597/597 [00:00<00:00, 1292720.44it/s]\n",
      "100%|██████████| 597/597 [00:00<00:00, 2210061.33it/s]\n",
      "100%|██████████| 597/597 [00:00<00:00, 1920245.01it/s]\n",
      "100%|██████████| 597/597 [00:00<00:00, 485083.20it/s]\n",
      "100%|██████████| 597/597 [00:00<00:00, 2047423.95it/s]\n",
      "100%|██████████| 597/597 [00:00<00:00, 2032467.12it/s]\n",
      "100%|██████████| 597/597 [00:00<00:00, 791403.13it/s]\n",
      "100%|██████████| 597/597 [00:00<00:00, 2084928.80it/s]\n",
      "100%|██████████| 597/597 [00:00<00:00, 1902735.17it/s]\n",
      "CPU times: user 2.95 s, sys: 191 ms, total: 3.14 s\n",
      "Wall time: 1.92 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from sklearn import utils\n",
    "for epoch in range(30):\n",
    "    dbow.train(utils.shuffle([x for x in tqdm(train_dec.values)]), total_examples=len(train_dec.values), epochs=1)\n",
    "    dbow.alpha -= 0.002\n",
    "    dbow.min_alpha = dbow.alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vec_for_learning(model, tagged_docs):\n",
    "    sents = tagged_docs.values\n",
    "    targets, regressors = zip(*[(doc.tags[0], model.infer_vector(doc.words, steps=20)) for doc in sents])\n",
    "    return targets, regressors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train, x_train = vec_for_learning(dbow, train_dec)\n",
    "y_test, x_test = vec_for_learning(dbow, test_dec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "pandas.core.series.Series"
      ]
     },
     "metadata": {},
     "execution_count": 61
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "training_list = []\n",
    "for tag, text_val in zip(y_train, x_train):\n",
    "    listToStr = ' '.join([str(elem) for elem in text_val]) \n",
    "    training_list.append([tag,listToStr])\n",
    "\n",
    "testing_list = []\n",
    "for tag, text_val in zip(y_test, x_test):\n",
    "    listToStr = ' '.join([str(elem) for elem in text_val]) \n",
    "    testing_list.append([tag,listToStr])\n",
    "\n",
    "deceptive_training = pd.DataFrame(data=training_list, columns=[\"deceptive\",\"text\"])\n",
    "deceptive_testing = pd.DataFrame(data=testing_list, columns=[\"deceptive\",\"text\"])\n",
    "type(deceptive_training['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "deceptive_training.to_csv(r'/home/zan/Desktop/dl_autoencoder/docs/autoencoder/deceptive_opinion_autoencoder/deceptive_training.csv', index=False, sep=',')\n",
    "deceptive_testing.to_csv(r'/home/zan/Desktop/dl_autoencoder/docs/autoencoder/deceptive_opinion_autoencoder/deceptive_testing.csv', index=False, sep=',')"
   ]
  }
 ]
}